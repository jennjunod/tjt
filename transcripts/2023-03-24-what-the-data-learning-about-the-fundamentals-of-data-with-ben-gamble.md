---
showLink: "https://www.youtube.com/watch?v=RR_2YpzUYFI"
channel: "Jenn Junod"
channelURL: "https://www.youtube.com/@jennjunod"
slug: "what-the-data-learning-about-the-fundamentals-of-data-with-ben-gamble"
title: "What the Data?! Learning about the fundamentals of Data with Ben Gamble"
publishDate: "2023-03-24"
coverImage: "https://i.ytimg.com/vi/RR_2YpzUYFI/maxresdefault.jpg"
---

This is a transcript with timestamps of a technical conversation.

Write a one sentence summary of the transcript and a one paragraph summary.
  - The one sentence summary shouldn't exceed 180 characters (roughly 30 words).
  - The one paragraph summary should be approximately 600-1200 characters (roughly 100-200 words).

Create chapters based on the topics discussed throughout.
  - Include timestamps for when these chapters begin.
  - Chapters shouldn't be shorter than 1-2 minutes or longer than 5-6 minutes.
  - Write a one paragraph description for each chapter.
  - Note the very last timestamp and make sure the chapters extend to the end of the episode

Format the output like so:

    ```md
    One sentence summary which doesn't exceed 180 characters (or roughly 30 words).

    ## Episode Summary
    
    tl;dr: One paragraph summary which doesn't exceed approximately 600-1200 characters (or roughly 100-200 words)

    ## Chapters
    
    00:00 - Introduction and Beginning of Episode
    The episode starts with a discussion on the importance of creating and sharing projects.
    
    02:56 - Guest Introduction and Background
    Introduction of guests followed by host discussing the guests' background and journey.

    ## Transcript
    ```

TRANSCRIPT ATTACHED

---

[00:00] you you
[00:04] you you
[00:08] you you
[00:12] you you
[00:16] you you
[00:20] you you
[00:24] you you
[00:28] you you
[00:32] you you
[00:37] you you
[00:41] you you
[00:45] you you
[00:51] you you
[01:01] you you
[01:11] you before I undid the
[01:20] before I undid the before I undid the
[01:31] before I undid the before I undid the
[01:45] before I undid the before I undid the
[01:57] before I undid the before I undid the
[02:05] before I undid the before I undid the
[02:17] before I undid the before I undid the
[02:25] before I undid the before I undid the
[02:37] before I undid the before I undid the
[02:51] before I undid the before I undid the
[03:03] before I undid the before I undid the
[03:15] before I undid the before I undid the
[03:27] before I undid the before I undid the
[03:39] before I undid the before I undid the
[03:51] before I undid the before I undid the
[04:01] before I undid the before I undid the
[04:13] before I undid the before I undid the
[04:25] before I undid the before I undid the
[04:33] before I undid the before I undid the
[04:48] before I undid the before I undid the
[04:55] before I undid the before I undid the
[05:05] before I undid the before I undid the
[05:17] before I undid the before I undid the
[05:30] before I undid the before I undid the
[05:39] before I undid the before I undid the
[05:52] before I undid the before I undid the
[06:02] before I undid the before I undid the
[06:10] before I undid the before I undid the
[06:20] before I undid the before I undid the
[06:30] before I undid the before I undid the
[06:39] before I undid the before I undid the
[06:49] before I undid the before I undid the
[06:59] before I undid the before I undid the
[07:12] before I undid the before I undid the
[07:21] before I undid the before I undid the
[07:34] before I undid the before I undid the
[07:44] before I undid the before I undid the
[07:56] before I undid the before I undid the
[08:10] before I undid the before I undid the
[08:18] before I undid the before I undid the
[08:31] before I undid the before I undid the
[08:38] before I undid the before I undid the
[08:50] before I undid the before I undid the
[08:57] before I undid the before I undid the
[09:05] before I undid the before I undid the
[09:18] before I undid the before I undid the
[09:26] before I undid the before I undid the
[09:36] before I undid the before I undid the
[09:48] before I undid the before I undid the
[09:59] before I undid the before I undid the
[10:09] before I undid the before I undid the
[10:19] before I undid the before I undid the
[10:29] before I undid the before I undid the
[10:42] before I undid the before I undid the
[10:55] before I undid the before I undid the
[11:03] before I undid the before I undid the
[11:16] before I undid the before I undid the
[11:23] before I undid the before I undid the
[11:36] before I undid the before I undid the
[11:46] before I undid the before I undid the
[11:55] before I undid the before I undid the
[12:05] before I undid the before I undid the
[12:15] before I undid the before I undid the
[12:24] before I undid the before I undid the
[12:34] before I undid the before I undid the
[12:44] before I undid the before I undid the
[12:53] before I undid the before I undid the
[13:03] before I undid the over there, go get it. But over time, as per always, everything sequential is faster, but still
[13:17] it retains the name random access memory, and it is what's often referred to as memory or main memory on your machine. - Okay. We're going to go with okay. Keep going. I'll go back to it. - No worries.
[13:32] So the kind of thing is, if you imagine like you're now writing some code, right, and let's say you make an array. So let's just imagine over here, so you've got, I don't know, let's say we're going to
[13:44] have a list of votes. We're building some voting application. So votes is now an array. And that, and let's say equals, so let's give it some things. So it's going to be an array list,
[13:59] because apparently I'm doing Java today. - Can you make that like way bigger, please? - Oh, yes. Let's make it way bigger. - Thank you. Perfect. - So let's do it with a bit more of a
[14:14] syntax that makes more sense to at least me. So votes is, so we're going to have an integer array of votes, and it's going to be a nice long array. So the key thing here
[14:32] is that each of these things is currently going, as I sign this out in most languages, this will end up in memory. It's not going to be up, it's what's known as the heap in most coding languages.
[14:42] So in most kind of C things, like C, C++, or other kind of like strongly typed, nearly strongly typed, but compiled languages like this, you end up with the idea of the heap and the stack.
[14:52] These are all kind of RAM constructs. And in RAM, you end up with like, if you assign out of an array, and you keep adding items, you're going to keep assigning a bigger and bigger chunk of RAM.
[15:02] Arrays are really fast for this reason, because it's a big sequential chunk of RAM. And what happens here is, as you start, so this is in RAM, and as you start, let's say, loading data,
[15:14] some historic data from your disk drive, so let's say you open a file and load it in, you're going to load it into this array, which is in your RAM. And now you can act on it, like count the number
[15:23] of votes total from the different places. And do the counting, let's say you're trying to count them up, you're going to go from your RAM, into your cache, and then run like an accumulator on
[15:35] it. So you've got, if you had a quick for loop through this, let's say, for vote in votes, now you have it, and then you say, total is plus equal to the vote count on that point.
[15:56] Now what happens is, total will keep adding up every vote as we go through. But this operation here, that plus sign, is happening from cache. So we load the vote number into cache, we do the
[16:08] addition, and then we return it back to RAM, when we save this total. So this is where, basically, everything has to go through the various stages above it, before it actually does anything useful
[16:19] for us. So basically, if you imagine the problem is always going to be that I can't touch the data until it's touched my cache. So it's going to go up through the cycles before it actually becomes
[16:33] useful to me. And as these things get progressively smaller, so let's say a cache is in the order of megabytes, or single digit megabytes, this is in the order of low double digit gigabytes,
[16:46] like your laptop probably has 16 gigs of RAM, or 32 gigs of RAM if you're in a good place. And then SSDs, we're talking single digit terabytes on your local machines.
[16:56] I've got a couple of 10 terabyte hard disk drives from yesteryear lying around still somewhere. And then when you're down a bit level, a bit lower, you're into the, you know, oh, you might
[17:07] have a kind of 100 terabytes, or you have terabytes of storage external to yourself. Right, multiple, like, imagine a Dropbox, like, you know, Dropbox account comes with, like,
[17:18] two terabytes plus. I feel, okay, I'm jumping around. So if we go with this, let's actually skip back a bit. Because what happens here is always is what goes on, it comes back to this
[17:35] kind of model over here. So we start with the idea of, you know, acting on data. So let's take, give me a use case, which you wrote a piece of programming on.
[17:44] Um, I mean, one of the ones that I was trying to set up, actually, when I was first interviewing with Ivan, was being able to set up something I called a Tweety tag app of tagging people
[18:02] automatically if they wanted to get a notification of a Twitter space coming up. Ah, so in this case, we have various pieces of information, we need to know the list of people
[18:14] who, you know, list of spaces from somewhere. So let's actually do this, because this actually makes more sense this way around. So let's start with only a list of spaces, make this big,
[18:27] because apparently the maximum size is not actually that big. So list of spaces, list of spaces, list of users, and then we've got the idea of a map or some description of users
[18:45] to the spaces. So these are people who are interested in one or the other. Let's make that a bit bigger. Oh, actually, let's make it a bit smaller and zoom in.
[19:01] Because then I could actually use their font sizes without it breaking. Things you learn as you do things live. Right, and let's spell that right.
[19:11] Okay, so you think about these, these are the kind of three main things you're dealing with, isn't it? We've got a number of users, a number of spaces, and the ability for a user to know
[19:25] about that space is basically saying, you know, I know that user one is interested in spaces A, B and D. User two is interested in spaces C, D and Z. And if you imagine then you've got this kind
[19:38] of like, basically a grid. So if we imagine a lovely square space, and we draw some lines on it, so this is our grid of learning shortcuts as you go along.
[20:00] So this is our users down one side. So users, and then as you go across, there'll be spaces. So the idea here is that as you say,
[20:22] like, user A, user B, user C, user D, and then these are the various spaces they're interested in going across here. So the idea really is that then what you do is you map the subscriptions to
[20:40] the potentials. But the problem here is that as this list gets very big, like, let's say you no longer have 10 users, which is very easy to manage, you might just store that in array,
[20:50] you end up, let's say, saying, I now need to deal with all of Twitter. So some number of tens of millions or hundreds of millions of users. Now, that actually becomes problematic, because
[21:01] once these kind of storage things get very big, it stops being actually worthwhile storing it all in one place, because even though you'd like to store it all in RAM, 100 million items, let's say,
[21:16] so let's say 100 million, each of these, so 100 million users, each of these users has a little bit of information attached. Let's say total size, so even just their name is probably,
[21:30] you know, a fair number of bytes, might be 20 or 30 bytes, just their name or their email address stuff. So you start getting quite large. And when you multiply a byte by a million, you're already
[21:39] at a megabyte. And then we've got 100 million users in Twitter, let's say, so you're at 100 megabytes of just user information before we add anything other than their name. Now, a user might
[21:50] even have a couple of megabytes of information each. So now we're maybe, you know, more than gigabytes, we're now into petabytes and maybe, no, terabytes, maybe petabytes of actual just user
[22:01] data, right? And we can't put that on one computer, it's just too much information. Really quick, in case anybody didn't know this, and I feel like I only know this because I used
[22:14] to work with physical devices, of 1 kilobyte, 1024 kilobytes go into a megabyte, 1024 megabytes go into a gigabyte, 1024 gigabytes go into a terabyte. Terabyte? Terabyte, yeah. Okay. I was like,
[22:35] it sounded weird. Some people say terabit, terabyte, all of them. Bit and byte actually is when we get different, so we'll just stick to bytes. I know, I know, I get burned by this one
[22:46] regularly because I forget which is which. And I believe bits is when you get to the powers of two bytes, bits or byte, one of them is powers of two, one of them is round units. And I can never
[22:57] remember which is which. I'm sure the chat will help us on that one. So as we kind of go down, this is like kilobytes for the Bs, little Bs on this one. So as we start going down here,
[23:14] single users are probably into the kilobytes range. Multiple users, you're probably doing a couple of megabytes. We start getting to reasonable size applications, you're into
[23:22] gigabytes. And when you get to big global applications like a Twitter, you're into terabytes and petabytes of actual information just to handle your users. And if we remember back to
[23:33] our little model about how much space various devices have access to, right? You can see how much your laptop might have. So my laptop here has a one terabyte SSD. That's pretty big considering.
[23:49] But if we're starting to look into multiple tens and hundreds of terabytes, that's unfeasibly large to fit on one machine. Well, it's unfeasibly large to fit on one machine, particularly into petabytes.
[24:01] So instead, what happens is because you need to have this data stored, and you probably want it on more than one machine. So if one falls over, you still got something else. You end up saying,
[24:11] I need this somewhere else. So this data cannot live on the machine that's doing the math directly. So now you've got this idea because you've got one user to your users, which are now in the
[24:22] multiple terabytes. And now per user, you have an additional field, which is going to be the stream data. So really quick, you gave us an example of like a very large one, like Twitter
[24:35] would be a very large application. What is something that we may be able to associate with that may be like in the gigabytes? Okay, so let's talk about let's say we're making
[24:51] a game, right? I've built a lot of games in the past. So a game, if you imagine how big a game is when you download it. So Doom 2016, I remember downloading and being surprised at it being about
[25:01] 60 gigabytes. So that's kind of like a lot of information. It won't fit into your RAM at once, unless you have a wonderfully powerful machine, instead it's on your disk. Because what happens
[25:12] there is there's loads of these big, most of it comes down to assets. So these are the things you can see in the game or things you can hear in the game, the music, the pictures, like the textures,
[25:22] which go on to the 3D objects, the 3D objects themselves, the cut scene videos, these are all big chunks of data, right? And they're all compressed up and stored inside on your disk
[25:34] drive somewhere, whether it's an SSD or hard disk drive, it's kind of a bit moot. Then what happens is, as the game starts running out, is as you enter a level, it has to stream all that data
[25:47] off your hard drive into your RAM, and then through your CPU or your GPU to get it on screen. Because you can't see it when it's just a pile of ones and nots, you've got to actually render it.
[25:59] Okay, could you put that into, because we have the hard drive, and that's the SSD, HDD, okay. Yes, yeah. And then what is the GPU again? So the GPU would be your graphics processing unit. So up
[26:19] here, we're into CPU land, right? So this line here, and basically here is where your CPU lives. So if you don't, a GPU is just the graphics version of it. So this is like a graphics card,
[26:30] or if you don't have one, it's in your CPU as well. Okay, so you know how sometimes you buy like a, you know, like a video card or a MD card, those are new processing type units, which use
[26:42] exactly the same kind of logic as your CPU, just architected to fit differently. So the important thing ends up being that, let's say in a game like Doom, you still need to pull all those level
[26:55] information, put it somewhere, and then load it into a place that can process it before it can happen. Then when you press left on your screen, and your character needs to shuffle to one side
[27:05] to the other, you need to be able to process that, then change where the model which represents your character is, and move them in the world. So if you imagine that has to happen either at a GPU
[27:16] level or a CPU level. So that has to go right at the top here, in this kind of cache zone here. Oops. So the kind of problem here is that like, that data is tiny. The I press button left is a
[27:33] tiny piece of data. It's in the single digit bytes. So you can easily put that in cache and say, use that information to change a lot of other information.
[27:41] So when we talk about rendering, and from my understanding, rendering is just, is rendering the same as buffering? It is part of it. So buffering is part of rendering. If you
[27:57] imagine rendering is literally just drawing dots on your screen, very high density of dots, so you can put a picture up, you draw into a buffer. And then you then draw into the next buffer,
[28:09] and then you swap them over and over again. A buffer is a piece of memory. So actually, this goes very nicely into our little diagram over here. So what happens here,
[28:19] and this is a very nice segue, by the way, is if you imagine the fact of like, everything that takes time and is actually important that it happens very quickly has to happen right at the
[28:29] top, okay? So what happens here is your, in your GPU and your cache in there is you have your buffers. So what happens is when you want to draw your next screen, you fill the next buffer.
[28:43] While the first one is being drawn, while the first one is being transferred up to your monitor, you're filling the next one, right? And it's like an area of memory. And then as you continue,
[28:52] you'll keep swapping them around and around and around. If you imagine it's like buckets, throw one on screen, fill the next one, while someone else is filling the buckets behind you.
[29:00] So bucket on screen, pick up the next bucket, bucket on screen. But the person down the line is going, fill the bucket, fill the bucket, fill the bucket, as they come back around.
[29:08] And that's really what's exactly happening, minus the analogy of buckets. Okay, and this is going off on a random past comparison of what about CDs?
[29:25] Because I remember like CDs, like my disc walker. Yeah, yeah, yeah. I was like, whatever they're called, and you would put it in and you'd have to wait. You'd wait.
[29:44] Same with PlayStation. And you'd wait. Yes. And the wait. So Discman are actually one of the better examples here, because what happens is
[29:55] particularly when you stored audio on a CD, you stored it in consecutive data bytes going around and around and around and around and around. So what it would do is it would span up really fast
[30:05] is it would read them in order. Okay. Now, if you remember how on old discs, old CD players, it took a little while to get from one track to the next. That was literally because the head
[30:16] had to, the read head had to move to where the start of that track was in physical space. The motor had to go and start reading. Now, do you remember how they used to skip all the time if
[30:26] you're walking around with it? So this is where for a while, like this is because they were continuously reading as they were playing, which meant they couldn't seek very fast because they
[30:37] couldn't hop around very easily. So what happened is if you just jolted it, it would miss a second of reading and it would just go or jolt out for a second because the data would just be lost.
[30:48] So do you remember when they brought out the anti-jog system for a while on discs? No, no, I didn't get to upgrade that often.
[30:57] So I didn't either. However, I watched a lot of adverts on TV. Okay, fair enough.
[31:02] No, I was very much the person who like really also used to read all the boxes in the electronic stores and there's like anti-jolt system. And what this was, it was a buffer,
[31:12] right? So what it did was it used to load up, you know, the next 30 seconds of music into a buffer in RAM effectively, off the disc. And therefore it always had a 30 second window. So if it lost
[31:22] some data, it could quickly go, oh crap, get more data. And the fact that it missed a bit of reading because it had a bit of a buffer window to catch up.
[31:32] This is making sense. I'm using this and I'm going to go back to computers as well, because this will help with databases.
[31:43] It's all the same. It's all the same. The plodding joke is- I know they're the same, like, but it doesn't click. Okay. So how that's reading data,
[31:53] that makes sense. A CD makes sense to me. Now, I would say that the next part that doesn't quite make sense is working on figuring out how to say it. If we are going from like, I'll just go in
[32:19] the progression. Napster was the thing, was the cool thing and downloading songs. And okay. So of course we talked about it and I'm going to go back. If we did not have enough CPU to download
[32:39] songs- Actually, let me flip it around for a second. You've got a song on Napster, which you want to play, right? So your CPU could play that. So the problem is it's on Napster,
[32:53] Napster is far away. And back when Napster was a thing, I had a dial up modem. Did you have one of those? I did, I did. Which was on the state side. But I had an equivalent in the UK, which was
[33:07] equally slow and terrible. So like what happened was, you remember like when you had that and you press go on the download and you do download a little bit and then it would start playing and
[33:15] it would stop and it would download a bit more and it would play and stop. Yes, it was very aggravating. It was because that, so basically this is where buffering used to come in, of course,
[33:25] is the same thing. You basically fill up a buffer and you play as much of the buffer as you could. Now, your CPU was plenty fast enough to play any song. So when it fully downloaded, it played
[33:33] seamlessly, right? But your network was so slow that it couldn't keep up with the play speed, right? Because you couldn't download it fast enough to continually play it. Like if you imagine
[33:46] we still had internet that fast, we couldn't have Spotify. We'd have to pre-download all the songs. So the CPU was plenty fast enough. Your hard drive was plenty fast enough, but your network
[34:00] was too slow to handle it. This is why network is that those little squiggles are about how much slower network is. These days, everything's a lot faster, but it's still comparatively slow.
[34:11] And Bro Nifty was talking about buffer from local file or from an upload in the browser. Buffer is the object used in a stream, which is making sense, but I guess,
[34:25] okay, let's say I have the best internet in the world now, but everything else is the same for Napster. We're going to go with that. So take one component out of the internet. So if it downloaded
[34:42] very quickly and I had to worry about the download space, that's not the CPU, that's the RAM? Oh, it's all of it. So your CPU has nowhere near enough space to hold. Let's just scale up a bit.
[34:58] We're now in the modern world. So rather than Napster, we're talking Netflix, big 4K movies, right? Because it's the same problem because the pipes have just got fatter. So now you've got a
[35:08] 4K movie coming along, right? You're talking megabytes per second here. Your CPU can't really hold more than a few seconds of movie at a time, right? Like a 4K movie, if I remember rightly,
[35:19] it's about a hundred gigabytes. No, it's enormous. Like it's crazy big, right? So what happens is you can't even fit all that in RAM. So you're loading. So let's say it's on your local drive.
[35:32] So you've downloaded the movie, you bought it on Amazon Prime. We're being good citizens today. I know it's a change. Particularly from the Napster days, but hey.
[35:42] We learned our lesson. We upgraded from Napster, we've learned our lesson. Yes. And so what happens is, as well as so big, you load up a chunk and it starts playing,
[35:53] right? Only a little bit of a time goes into CPU and it's being pulled out of RAM and your RAM is pulling it and it's being pulled into your RAM piece by piece in big chunks. So you put a big
[36:02] chunk into RAM and that tiny chunk of that chunk goes into your CPU and it goes up the track in smaller and smaller pieces until it can be put on your screen. But because of the higher up the
[36:11] stack you go, the faster everything goes, you're actually okay. What goes wrong is when something can no longer keep up with the level above it, right? So now let's say you're streaming that 4K
[36:23] movie, right? And we're doing it on a bus or a train because trains and buses are slow and Netflix is on a, you got Netflix on your phone. We've all done it. It's lovely, right? Best things to do on
[36:35] a train, watch Netflix, but you know, I've ever been on an Amtrak, you don't always get a good signal all the way through, do you? So let's say you lose signal. Now you've got some of that data
[36:47] locally because, you know, you loaded up half of season one before you left the house, but you got to the end of what you had loaded partway through that long Amtrak trip. And now you're going into
[36:56] a tunnel. So you've got about, let's say, 10 minutes of pre-cached up stuff on a buffer locally. This would probably be on disk to be honest, because there's just so much data. And then what
[37:06] it'll do is that'll be loaded into RAM steadily. And then it'll try and keep playing and playing and playing until it runs out. So if it runs out- I'm going to pause you right there because FedEx
[37:16] is here. Y'all, I ordered stuff and they make me want to sign it. So I will be right back. Hopefully. [Pause for group work]
[37:32] [Pause for group work] [Pause for group work]
[37:52] [Pause for group work] I'm going to take this as a brief opportunity to draw more diagrams.
[38:00] [Pause for group work] [Pause for group work]
[38:22] Hello again. Only like three more boxes left to go today. I think this one is my watch. Yay. Okay. Put that aside and let you draw your graphics.
[38:44] This is basically what's going on here. So at every level, so basically the idea here is we're loading over the network. We're loading into RAM. Oh yay. I actually wrote down for next stream for
[38:59] us to look up visuals like this now that it's starting to make sense. So good thinking. Thanks. And a convenient moment for me to do it before I- well because I find it difficult to draw while
[39:11] talking. Oh that's a good time because I just got a box. Let's open the box. SSDs. So okay, I've caught up now. The main thing was that we're trying to think through
[39:25] what to draw. So this is your SSD. This is your RAM. Wow. Oh yeah. So the thing you often forget about this tool is because everything's actually just an SVG, which is a really long string.
[39:43] If you paste it inside a string box, you get the full string of the SVG actually in the box itself. Which is something I didn't realize for a while.
[39:52] Okay. So now we have our little diagram. So over here we have the network. So we can always assume the network is effectively infinite
[40:05] because it's someone else's computer and they have more stuff than us. But as you kind of go down up these kind of chain towards the CPU, the amount of data you can put
[40:19] in any location goes down by an order of magnitude nearly always or even more. But its actual speed goes up so much faster. So you can keep filling it very quickly. But it's like the buckets, right?
[40:33] So there's a swimming pool over in the corner which you're now trying to put out. So you're basically trying to shovel water onto a fire, right? The last bucket is very small but very
[40:41] easy to throw. And the next, but then down the chain, you're filling that bucket from, you know, a 30-gallon drum next to you, right? Which is still okay to fill but not very easy to move around.
[40:55] And then there's a swimming pool next to that, right? And you're pumping water from the swimming pool into the drum and the swimming pool is being filled by a tiny hose but it's from the water
[41:05] main. Swimming pool is your local disk drive on the SSD. The network is your little hose spraying the water and it's going up to your CPU which is you holding the last bucket throwing it.
[41:17] And each of these buckets, like the SSD, is a much bigger bucket but it's going to pour into RAM, the smaller bucket, slowly.
[41:31] So this, so the loading here is, this is the slowest load. Okay.
[41:37] It's down here on the load from the network. This is actually quite slow, right? So this is a very narrow arrow. This arrow, and they actually, the arrows go up,
[41:45] they get thicker and thicker. So this is a medium thickness arrow, right? And this one up here is actually a really thick arrow. So let's make this one a dotted line because that's actually more
[41:57] realistic, right? Let's make this one, actually let's just duplicate this arrow because fundamentally this arrow is really, really, really fast, right? So this is a really weak arrow up into your SSD.
[42:11] In modern days it's better but it's still pretty bad by comparison. This arrow here is pretty damn fast and this one is insanely quick.
[42:18] Okay. So we end up with a simple fact of we can nearly always read everything in RAM from the CPU. So
[42:29] that is kind of like the overview. And to kind of put this in some context for some speed, I'm gonna give you some numbers finally. So this is a lovely visualization we talked about a little
[42:37] while ago before we went on air, which is about the latency numbers. So we talked a bit about kind of rough equivalency of size and scale, but let's talk about actual performance numbers. So little
[42:50] black squares are nanoseconds and referencing L1 cache is only half a nanosecond. So it is rather insanely fast if you think about it. You're actually down to the limitations of the speed
[43:04] of light and some annoyances around quantum mechanics to make that actually faster. That literally can't really be much faster. If you look, there's something called Dennard scaling,
[43:15] if you want to look into that, which kind of shows you where we're starting to reach the limitations of making things physically smaller so they have to go less distance on the CPU itself.
[43:26] Now, as we go down into the actual cache itself, because it's leveled, the cache kind of ends up at like level two and level three, but we're still in the order of single digit nanoseconds.
[43:38] So then 100 of these black boxes is one blue box. And a main memory reference, this is going into a RAM for a one-off read, is effectively 100 nanoseconds. And that is assuming you're going
[43:55] to something which is relatively easy to get to. So it's between 100 and 500 times slower already than reading something in cache. So that's the kind of two plus orders of magnitude slower.
[44:10] Then as we go down, this is just some comparisons of how long it takes to do stuff. The thing is, once you're actually getting things in memory, doing work on them is not that bad.
[44:21] Like compressing a kilobyte with a compression algorithm, three microseconds, not too bad. You know, that's only effectively 30 references due to some tricks that go on behind the scenes.
[44:31] So then we get up to the green boxes, which are, once again, 100 times slower again, is to send it over, let's say, a network. This is a tiny file over a very fast network, right?
[44:45] A tiny file is 10 microseconds. Reading anything from an SSD takes a little time as well, but any read from an SSD takes a little bit of overhead at 150. But when reading sequentially
[45:01] from an SSD, as in reading in a line, you get pretty damn fast. So it's only a quarter of a millisecond to read a big chunk. And now you start thinking in terms of actual milliseconds.
[45:12] So inside the actual data center, let's say you're using a machine inside GCP, so inside Google Cloud Platform, and you're accessing another machine inside Google Cloud
[45:22] Platform on their super fast internal internet. And that's half a millisecond. But when you actually start to use the real internet, as in you to the server in GCP,
[45:35] let's say, you know, you're in Colorado, I'm in Cambridge in the UK. So we're talking about over here. So we go from microseconds to milliseconds, tens of microseconds to milliseconds. And between
[45:49] us, it's probably 100 to 200 milliseconds of latency is the minimum we can get due to the speed of light and the quality of the cables, right? So we've gone from nanoseconds to hundreds
[46:02] of milliseconds, which is quite the difference. It's over a million times in difference in performance. So if you imagine every single bit you send me, which I can see you on my screen,
[46:16] is 100 million times slower than the bits of my own video. And that makes sense why, especially post-COVID, I think it's a little easier for people to kind
[46:30] of start to comprehend this, because your video may look super clear on your computer, but over Zoom, you know, other people are seeing it grainy.
[46:41] Yes. And that is where we kind of like, we trade off the fact we can only send so much, so we compress the video, we drop bits and pieces of the video, so we can just send what we need to
[46:52] render it. So this is where we kind of come down to what, back to the kind of the data argument of like how we start with this access pattern stuff, and we end up with a combination of we can do the
[47:04] thing we want to do right now, or we can basically do it later in bigger chunks. So this is where that kind of like streaming of data versus like chunking up data to do it later comes in.
[47:15] So like this is, so use a stream versus batch analogy to go back to some of the Kafka stuff. A batch analogy is downloading the song from Napster before you play it. And the modern
[47:26] streaming equivalent would be Spotify. Because you play it as it goes. I, yes, everything you're saying like makes sense. I think this is one of the things I have a hard
[47:41] time with of where I can comprehend what you're saying, yet if I were to try to describe back to you what you just said, but even like the CPU or like the funnel from the website to the down below
[48:03] for that one that you referenced from the network or the differences, like they still get stuck. And I'm like, am I stuck on words? Am I stuck on a concept? What am I stuck on? So on that note,
[48:18] just keep us moving forward. Cause it could just be one of those things that just needs to marinate a bit before it clicks. What, like, so why did we come up with databases and what,
[48:31] I get that. And you, you alluded to this earlier that computers can't hold all these databases, especially like if you have for the Tweety app that we looked at, like it would have petabytes
[48:48] of data just by usernames. Cool. Just user profiles. Yeah. So let's kind of like, let's kind of go backwards through this little diagram and we'll actually get to why databases are really
[49:03] interesting and why they exist. So particularly in this one, this is about access of data, right? And occasionally how much read. So this basically tells us that reading data is expensive
[49:16] if it's far away. But if we can basically read sequential data, it's not as bad. Now, if we skip back to the kind of this, this lovely diagram here, which is actually probably the best
[49:31] diagram for everything we're going to talk about, to be honest. And really quick, because I get stuck on words, y'all. We've talked about this. So I'm just letting everybody else know
[49:43] sequential means that it's going in order. I'm Googling at the side. I figure, you know, if there's a word that I don't know or doesn't make sense, maybe somebody else doesn't. So just
[49:54] saying those ones out loud when I Google them. Now, please stop me about explaining any word. I have a bad habit of using a word with six syllables where a word of one will use will do.
[50:05] I'm really bad at thinking of understanding things in context. So like, I could have put two and two together that I'm like, okay, sequential probably means just going through a
[50:18] motion, but not necessarily in a series of steps or a specific steps you need to go in. So that's why I was pausing it. I was like, what you said made sense. It was more of, I need to break the
[50:31] habit of doing it out of context, or it will not make sense if somebody is using it in a different context. - Yes. Now, if we go back to that CD analogy we talked about a while ago, right? Now,
[50:44] your CD player from the '90s, which we all remember very fondly, me specifically, you know, it's playing some music, and it took a while to skip between tracks, right? And that was
[50:59] just starting from what, 15 tracks on a good disc, right? And remember how long it took just to find the track you wanted, because they just had track numbers. They didn't have names originally.
[51:09] - That is true. - I know. So you're thinking, I want to listen to "Bye Bye Bye" by NSYNC, of course, because that is about the genre,
[51:19] that is about the era we're talking, right? - Earlier, earlier. "Spice Girl." - "Spice Girl." So it's, so, okay. So let's go "Wannabe" by the Spice Girls.
[51:31] So you're trying to find "Wannabe" on that original album, right? And you're going to skip past "To Become One," because no one likes that song either. Yes, I remember my "Spice Girls"
[51:44] being in the UK, you couldn't avoid it, right? So you want to find that zig-a-zig-ha, and you're going to go skip, skip, skip, skip, skip, because the tracks aren't named on the disc when you're
[51:54] actually looking at them. So the problem there is you have no way to know which track is which ahead of time. So you've got to search through them all, start them playing, listen for a bit, and go to
[52:05] the next one, because fundamentally, you don't have a good way to identify what is what. All right? So you need an index. So you look at the back of the disc box, you know, the jewel case it came in,
[52:16] and it tells you the order. Now, fundamentally, that plus the disc is a database. It's a way to look at data. You need a reference to it. >> What you're saying makes sense.
[52:29] >> Because all the database is, is a structured way of storing data, and a structured way to write and retrieve it. >> Okay.
[52:40] >> That's all the database really is at a very core, very basic level, right? And even if the index is something you've got to read with your eyes, and then you've got to plug, plug, plug,
[52:49] plug to get to the next track, you know, you want to get to Spice Up Your Life, you've got to go to track six, I think it was. This is very vague memories from a while ago. I might be skipping
[52:58] Spice Girls albums around right now, but hey. >> So really quick, and I know not everyone was able to join on yesterday's stream. I started setting up Postgres. So I'm doing it locally,
[53:13] and I had to set up the server, and then I had to set up the client. If I take the Spice Girls CDs reference, I was basically, every time I wanted to skip something, I was being the client,
[53:26] because the CD is the server, the back of the jewel case, tell me what song I want, and I'm the client telling it to skip. >> Yes.
[53:39] >> Okay. >> Very much. Exactly, exactly. The analogy is nearly perfect, but we'll go for it because it's so close to true that it's pretty
[53:48] much exactly right. >> I just want to let you know that I will tell people that you taught me about databases with Spice Girls.
[53:56] >> I am so happy to say that. No. And the key thing that comes out of this is the idea of three major concepts here, which is there's the thing interested in the data, which is
[54:08] nearly always a client. There's the place the data lives, which is a server, and the thing that runs it and manages all the queries, which is that little panel of next, next, next, next, next,
[54:20] and that number you see. >> It's a query. Okay. >> Yeah. The index, which is the back of the jewel case telling you where to go.
[54:27] The ability to find this data. Now, let's say Postgres is a great example here because it has all the kind of usual bits you'd expect in a traditional database system. It's got a client,
[54:43] which you can load up either as a piece of an SDK or a tool on your machine, so you can put it in code, or you can use it from a command line, or you can use a web tool like PgAdmin. Then we have
[54:58] a server, which basically is your database management system, which basically manages the database inside itself, which is like if you make a Postgres database and then tables inside
[55:10] it, that's all managed by Postgres, because it's all in one box, right? Let's say you make a table, and let's say it's Spice Girls tracks, right? So you have the first table, which is the first
[55:24] album, the second table, which is the second album. I can't remember. I'm gesturing around, so I can't quite Google the names of the albums yet. But what happens then is your rows in your
[55:36] database would be each song, and then you save song name, who sings, the length of the track, and then some details about it. Maybe the lyrics, right, for that track are the entries in the row
[55:51] of each Spice Girls track, right? Now, what happens is Postgres will automatically have a way for you to find this data. So let's say you want to go select from Spice Girls songs, right,
[56:04] and you want to select all the songs which are longer than four minutes, right? So you'll go select star from Spice Girls songs where duration is greater than four minutes, you know, whatever
[56:18] unit it's stored in. Now, to do that, it's got to go through all the rows and say, "Is this number bigger than that number?" Yes. Put it into the list to send back. Next number, next number,
[56:30] next number, next number, next number. The thing here is that because it is stored structurally, it knows where to look in that data to do that comparison for you.
[56:40] So now let's say that the Spice Girls had a million tracks. They have a lot, but not a million. And what happens here is you now know that people are going to search for tracks by how long they
[56:54] are, right? So like, you know, people are looking for tracks which are exactly this long or exactly that long. So you might index that exact thing. Like your dual case just had the name and the
[57:07] number, you might have an equivalent dual case that gave you a track length and then a name of the track. And that's what the database indexes sort of do. They give the ability to sort through
[57:20] things quickly to find the subset you need faster. So this is like the filtering system, basically. This is you going, "I like it when Scary Spice is the main singer. So let's find the ones with
[57:33] Scary Spice being the main singer." So you have a way to say, "These all have Scary Spice starting." I'm... where are my thought processes currently going? It's totally skipping ahead of, like,
[57:51] what you're saying makes sense. And it sounds like this would be row-based. Yes.
[57:58] Because Postgres is row-based, meaning that you put like, in the index, it would be like, song title, and then length, and then lead singer. And those are each going by row.
[58:15] Now, where I think I'm getting a little mixed up is taking something like what we're talking about. Like, if Spice Girls had a million songs, I think Postgres is one of the easier ones
[58:34] to understand because it's very close to conceptualize of something that many of us use without meaning to, like Excel or Google Sheets.
[58:43] Yes. But then we have, like, Clickhouse, where it's column-based.
[58:48] Yes. Instead.
[58:50] Yes. What?
[58:53] Ah, now we're into the app. So this is where, actually, how the data is stored becomes incredibly important. So Postgres is heavily built around the idea of you'll want to, you'll,
[59:04] you both want to read your data and write your data. So now, this is where the music analogy sort of breaks down a bit, because that's reference data. But now if you imagine that.
[59:15] Oh, but we could do, if it's read and write, Postgres is kind of like, you need to search a song and burn the CD.
[59:27] Well, actually, it's more like, I want to favorite these songs. I'm changing who I'm favoriting, let's say, because burning the CD is a bit of a, there's a reason I don't
[59:36] want to use that particular analogy, because I want to use it somewhere else. Only for that reason. It holds, but I want to use it somewhere else. It makes more sense
[59:44] somewhere else. So let's say you're now, you now have a voting board of who likes what songs and want to give people tags, like this song is fierce. This song is more girl power. This
[59:56] song is more Britpop era, like iconic of the Britpop era. You're giving tags to all the songs and lots of people are changing. Now, it's really important that that data can be
[60:05] written easily. And when you want to write a line, you don't modify everything else. You just want to write the line you care about. Right? So Spice Up Your Life is fierce, but not
[60:15] really iconic of the Britpop era. So you're taking that tag off, but you're adding a different one. And because it's a row-based system, when you access it, you pull that full row out,
[60:23] and then you change it, and you put just that row back. It's not very efficient in some ways to search, to do a big scan across every piece of data in that one field. However, it's very quick
[60:33] to find the one thing and change it. So this is the old bucket analogy. You pick up the bucket, you take the thing out, you put the thing in, very good. But it's very bad for saying,
[60:42] "What's in all my buckets?" Because you've got to pick up every bucket that's in it. Now, if we flip to Clickhouse, Clickhouse, if you imagine the storage, right? So we go over here,
[60:52] and we'll draw some diagrams. So this is the old row-based thing. So this is row-based. And as a heads up for everyone watching now, and for those who may be watching later,
[61:07] please let us know questions that you would like us to fill in in between these, because I'm going off on my own experience. I have learned a little bit about Clickhouse. I've learned a little bit
[61:21] about Kafka. I've learned a little bit about PostSQL. And as you've seen in other streams, it's easier for me to compare it to something I might have used before, or know a little bit about.
[61:36] So please let us know if we need to dig in deeper somewhere. So fundamentally, though everything, all databases have rows and columns, right? Because
[61:48] fundamentally, they're all very fancy Excel documents, if you want to look at it that way. Even Kafka?
[61:55] Oh, yes. I could make that argument. I could definitely make that argument. Okay. Save that. I will ask you about it later.
[62:03] So what happens here is it's really about which bit you care most about. Now, these rows, if you care really about the rows, so as in this data, conceptually, it's one big thing,
[62:14] but it's important you have that row together, like the Spice Girls track, the likes and dislikes, the thing. That's all really important it stays together, because it's going to change together.
[62:22] You want it in a row. But now let's go over here and say we want it as a column. Right? The U, because I can't spell today, apparently. Now, what happens here is that
[62:38] we are now saying we still have our rows, but we don't actually think that the data is often needed in its entirety. So instead, what we say is, rather than storing, let's say, let's say
[62:52] instead now we're now looking at play statistics for Spice Girls tracks on Spotify, right? So what happens here is we have,
[63:01] oh, I believe this is nice. Now what we have is the first column will be called the title of the song, or the unique identifier of the song, so the title, right? So every single play event which
[63:17] comes through is going to be the title of the song, so the unique identifier of the play event, the song it's being played, the length it's being played by, the user who played it,
[63:25] where they were in the world, et cetera, et cetera, and a few other columns going that way. Now, the point really here is that I want to know, let's say, the total number of hours
[63:34] Spice Girls were played for, right? So I know how much to pay them because it's Spotify, right? So this is an example of a query where, though it's been written to, I'm not changing that data
[63:46] ever, really, right? It's historical record, right? This is data which is, once it's true, it stays true, right? Like, how we classify a song based on who likes it and dislikes it,
[63:58] it's going to change over here. But over here, it's, I played it, so I played Spice Up Your Life yesterday for three hours on repeat. So three hours of play time, the idea of this play is,
[64:15] let's say, some magical number which is unique. This track is Spice Up Your Life, the user is Ben Gamble, and I played it in the UK, okay? So what happens here is there's a three-hour thing
[64:27] here. You happen to play Wannabe for five minutes yesterday because you played over once and you didn't stop it in time to stop it playing the first few minutes again. And every time this
[64:36] happens, going back in time, you have this long record. Now what happens is Spotify say, "How much do I pay the Spice Girls this month?" So they say, "For this last month, give me the
[64:45] total of play hours of Spice Girls." So what they do on a column-based database is they go, "Well, this is the database of the thing. I'll just sum up this column, please." And it just scans through
[64:58] that one column. We don't read any of this other data. -So the data sets for both examples are going to be different. It could be the same subject, but not the same data set. -So occasionally it is,
[65:13] but very rarely. So if you imagine what, so imagine this was the same. Let's say this was, I don't know, another example. Now let's say, if we flipped it, so let's say purchases, right? Like
[65:25] you want to buy something, right? We say, we might make a row in our transactional database saying, "Jen has bought headphones today," right? And now we say, "Jen has paid the headphones." We say,
[65:37] "Paid," over here. We say, "Jen has been shipped the headphones. Jen has not returned the headphones. Jen has claimed a warranty on these headphones," or the fields here. And there's a series of check
[65:45] boxes that can be unchecked and checked, right? And then you have your review at the end. Now, at any point in time, there'll be a state of this table, right? The only time, if you want to change
[65:57] anything, you need to know the current state and the current state only. But over time, as this table changes, what we can do is we might extract out the time, what happens, and dump it in a
[66:06] column. So at one point in time, you had just purchased them. They weren't in shipping yet. So the first row is, "Jen buys headphones." This is the current state of the headphones. The next
[66:17] line would be, "Jen has headphones. They're now in shipping." Event, event, event, event, with a timestamp next to each one. So then what we could do in analytics worlds, outside of this,
[66:26] when we don't want the data to change, we see we can now chart how long it took to say the average shipping from buy to ship was, because we can look down all the columns and work out what happens.
[66:36] Because we know how long, we can basically do a quick query to work out the difference in time between point A and point B. And we can then do something to say, "How many people returned their
[66:46] headphones?" So I can go count the number of returns. Because I don't know how many people, because in this column over here, in the road world, this is going to keep changing.
[66:57] It's not a very good example. I can't think of a brilliant example for this right now. You're giving me an idea. And I know that you talked about the possibility of coming back on
[67:07] the show again, which I think would be really dope, first off. But it's making me think just some things that I would like to write out and show to you to have you help me break down later
[67:22] on is the process of how you have the computer parts or pieces and have those go through and put it in a way that I might understand it and visual there. And then also here with data
[67:44] filled in, I'm saying it like that because I'm not going to go find a data set. I'm going to just see if I can understand a data set or create my own. But something that I did really want to
[67:58] ask you today that might actually fit in really well here on my website. So I have too many things going on. I don't have too many things. They all kind of relate. I have Teach Gen Tech. I have a
[68:14] podcast, shit you don't want to talk about. And they both, I want to set them up where they both have like their own individual page on my site of each episode where it has about the author,
[68:30] the graphic and a transcript and a link to go watch it or listen to it. Can those both use, and I want to have a blog on my website, just a regular blog, not structure for
[68:45] one of the shows, but an actual blog. So is it possible to use the same database for everything or would I need a separate database for each? So the golden rule for choosing things like databases
[69:04] and structures is always a matter of trade-offs. So in this case, because you're a single contributor to a website and what you really need is flexibility more than any single thing else,
[69:16] right? You want a database that can just do it all. So Postgres is a perfect choice here because fundamentally your data load is not that large and Postgres is pretty amazing at just being
[69:28] the workhorse of a website like this. However, if you wanted to start logging what everyone was doing on your website, let's say you want to track how long people look at, I don't know,
[69:38] each of your podcasts, all the different listening events, and you wanted to track how long people scroll down each of your blogs individually, that is where you wouldn't use Postgres for that.
[69:47] You'd probably use something else, classically a column-oriented database like BigQuery or Clickhouse. Because what happens here is every time one of these events happens,
[69:58] you only normally care about one field at a time. So the reason I've done the highlighting, like this, actually plays quite nicely here. So if you imagine you're displaying blogs, right,
[70:09] so if you have an index, let's say, that says, it's a time-based index, so you want to find the blog which is, let's say, June 15th last year. So you can index on that, so you find June 15th
[70:21] last year and you return the whole row which has the blog, the title, the picture, the whatever, the whatever, right? Chunk, data on screen, looks great. You want to make a quick edit,
[70:30] very easy to do that. It's Postgres, it's very good with edits. Now, let's say you were in an article, it trends on the front page of Reddit, right? Oh yeah, it would be great. And let's say
[70:43] your website held up nicely because you engineered it great. Now the problem then is you want to work out where in your website these people are going and what drew them in. So you'll need something
[70:53] with a good amount of analytics power to pull this apart. But now we've got multiple millions of events happening very quickly, and you want to be able to pull that data apart. Now, the first
[71:03] thing that's happening is you put into a column store like Clickhouse, one, it can handle that right volume pretty well because it's very well-built to just take that massive stream of
[71:12] data in. Now what happens next is you want to firstly, let's just say, is give me the various places people are coming from and the totals. So you say, I want to sum up all the different
[71:22] places websites people are referring from, right? So what happens is this red row is going to be the referrer, which is the website that people came from. And what this will be able to do is
[71:32] you'll be able to return the top five or six websites of where people came from, which is going to be Reddit, number one in a very large way. It'll be Twitter from people linking to it
[71:41] directly after they've read it. And then it'll be something like, I don't know, organic Google searches because you try trending really well. And this is where a column store works because
[71:51] you can search that just that column. And this is where, because this is now a lot of data, and this is the important bit, is because you don't, because you can only read so much data
[72:01] out of the disk on one time. So let's say you've got to the top of Reddit, you generate gigabytes of logs, gigabytes of these events very quickly. And if you did this a few
[72:10] times in a row, you'd be in the terabytes. So this is back to our lovely reading pattern thing is I don't want to pull terabytes into my RAM. I want to pull gigabytes. I don't want to pull gigabytes
[72:22] into my CPU. I want to pull megabytes. So I only want to search this one column. So this is where my information comes from, by the way. This is where I was going to get
[72:39] the burning CDs. So what happens is right into a column of database, like Clickhouse, is like burning a CD because it's very, very fast to read just the tracks you need, right? More
[72:53] often than not, when you're listening to a piece of music, you don't need to listen to three different tracks at once, right? You nearly always only care about one track of music.
[73:01] Then most column of databases, you only really care about each column on its own, which is the view count compared to one other column at a time. And the math you do on it is on one column at a
[73:13] time. So what happens is you want to sum unique events. So you scroll through that column very quickly, which means you can do the whole buffering trick we talked about here.
[73:21] We just go from-- we don't go over the network. We go SSD into RAM into CPU on that one column in order. This is when it's super fast to go in order. We're not hopping around the disk. We're
[73:32] not randomly hoping to find the data. There's no-- it's all sequential. It's all very quick, and it goes very nicely through. So this is why Clickhouse has such ridiculous performance for
[73:41] analytics-type queries, where you care about one or two fields at a time. Because it's like a burned CD track. And this is why it's very expensive to edit
[73:50] this data, because it's like a burned CD track. And if you remember the horribleness of the CD-RWs and things from back in the day and how much more they costed. Yes. Yes. Exactly the same
[74:03] mentality. CD-Rs, cheap. CD-RWs, weirdly expensive. I think a part that I'm also a little stuck on, and this is-- I definitely know that sometimes I get into the weeds.
[74:16] If we go back to the rows option-- or not rows, the columns example, and we're talking about people and traffic, does that mean to be able to do something like this? And this is,
[74:36] again, me in the weeds trying to just connect the concepts when I'm stuck on the in-between. Do you have to have a URI set up or something? Or URM, is that what they're called? When people
[74:49] know where the link came from? Okay. So a UTM, or the Unique Tracking-- That's what I'm working on. UTM, yes. Yes. So I can't remember what the acronym means because I have honestly not looked
[75:03] it up in about a year. I'm bad with acronyms. But this is-- so a UTM is really useful because what it does is lets you have additional knowledge, right? So if I have a UTM and I go out, and let's
[75:15] say I give a talk, and I put a UTM at the end of the link I give on to my-- to, let's say, a link to a sign-up page, what that means is I have a way to uniquely identify it came from this link
[75:28] I gave you, because that link was clicked directly. I copied that link into my browser, right? I didn't go from one website to another website. I just started on you.
[75:36] So then I need a UTM. If it's on Reddit, and I click on the Reddit link, right? If it has a UTM, that's great. I'll get that tracking information. But I'll know it came from Reddit because I'll
[75:47] know where the previous page was. I get that information automatically. So UTMs are really powerful because let's say all five-- let's say we have five people on our team. We all go out
[76:00] and share at various places in social, right? And we want to know roughly which one worked best for good-- for whatever reason. And let's say I did LinkedIn, you did Twitter, and some of our
[76:10] colleagues did Pinterest, Instagram, Facebook, and, I don't know, Myspace, because someone was feeling a bit retro, right? - Sentimental of the past.
[76:22] - Yes, and let's say that one actually did really well because it got reshared in other places, it turns out. Turns out the few people left on Myspace are really hardcore sharing people,
[76:33] and it went viral in many places. So that means that UTM probably went with it, and we'll know where-- even if it got reshared, we'll know the UTM was where it came from.
[76:42] - Got it. But if we didn't have the UTM, a website can still track at least where the traffic came from. - It got to the
[76:55] top of hacking. Let's say the one that was on Pinterest ended up on the top of Hacking News, right? And then what happens is it became-- it gets reshared on Hacking News. Now it looks like
[77:10] all the links came from Hacking News. Perfect, but we don't know where it came from originally. But if it has a UTM in there, we'll know that's the same link, as in it got shared from there,
[77:20] because the UTM is just-- it's another way of categorizing where it came from. - Okay. This is helpful piecing it all together. - So now let's look at the Columbus store thing,
[77:31] because if all this information comes in, now what we can do is we have a column for UTM and a column for where-- so let's say this is the UTM column in red, and this will be in-- green will be the
[77:42] previous website. So if this field is null, like no previous website, it's just a UTM field, we'll know it was from your talk on that QR code you put on screen.
[77:50] - Makes sense. - Now let's say that this field was green, and this one-- but it said-- let's say it all said Hacker News, but let's say that the UTM
[78:01] fields were all over the place. It means all of our various UTM links ended up on Hacker News for different reasons. However, if the one UTM is-- if it's all one UTM specifically, and it comes from
[78:16] various different places, we know that article got shared really well. But the kind of key point here is that because it's on a column store, we can do things like, say, we can-- let's say we only care
[78:28] about those two columns. We normally only care about a few things at once in a column store, because these might be a billion clicks over the course of six months. And what happens is we say,
[78:40] give me a summation of all the various things. So let's say all the different websites that were sent as information, and give me the various UTMs assorted with them and the count for each piece
[78:51] of data. So let's say I want to know the number of Jen UTMs from Twitter, the number of Ben UTMs from Twitter, the number of Francesco UTMs from Pinterest, and the number of Tibbs UTMs from
[79:08] Mastodon. These are some of our colleagues. And the idea here would be that we could then do the summation across them, and we might find out that turns out your following is not actually on
[79:19] Twitter like you think it is. It turns out it's all on LinkedIn, but it got re-shared. And because it's on a database, it's very quick to find this stuff out.
[79:30] So we can just do that in a single-digit milliseconds, or in single-digit seconds, we can get that answer out of even a very large Clickhouse table.
[79:43] Okay, this is going to take some noodling to wrap my head around, because I really like how you were going back to the,
[79:53] on showing how it goes through from the network upward and all of the buckets, and then into the databases. I still... Okay, so this is an interesting point to note.
[80:12] So now what happens is, let's say we are, so if we're actually using a database system, let's say, this is where it gets quite good fun. Let's say we're actually using a really big dataset.
[80:23] Let's say we're doing something like asteroid detection. Now the datasets we have, so I remember when I was at university, we were looking for what was known as like, we're looking for big events
[80:34] like gamma ray bursts and equivalent stuff, which is like when a black hole spits out more gamma rays than is sensible and might wipe out all life as we know it. Yeah, crazy events like that.
[80:45] - Okay. - Or supernova, or other things in the night sky. Now we have tons of this data. This is like
[80:50] petabytes of data, genuinely fun-sized big data. Now, our local machine can't handle all that, so it's all in some very large array of SSDs. So what happens here is we have
[81:02] lots and lots of these SSDs. Let's actually give them the logo, the name as well. So let's say an SSD like this, right? So actually we have, so this is over the network. We have a big pile of
[81:17] SSDs, right? They might be slightly better organized than this, but let's say we're an astrophysics department, so probably not, right? So they're all over there. And then what happens
[81:30] is we say, on our machine, I want to know, I want to find these gamma ray burst events or these supernovae, right? So the first thing I'm going to do is say, all that data is in some massive
[81:42] Clickhouse cluster, which is, let's say, 150 machines running Clickhouse, all with disks attached, right? All somewhere else, not on my machine, okay? And what I say over there is,
[81:55] give me, the first thing's first, let's say the count, because I want to know how big this data is. Before I even try and get something on screen, I want to get some rough guesses on numbers,
[82:04] right? So I say, give me a count of the number of events where the intensities, how big and bright the event was, is above, I don't know, some number of standard candles, which is a measure of
[82:16] brightness for astronomical things, right? But also that it was in the gamma spectrum. So the spectrum, the wavelength was below a certain number and the brightness was above a certain
[82:27] number. But what I want to do is count. So what I need to do is I send this to Clickhouse over my network, say, give me this. Clickhouse says, okay, and it scans through just those two columns
[82:37] and says greater than and less than on each column. So super fast on all the RAM and all the various machines and all those SSDs over the network. And I get a series of counts back from
[82:47] every machine, through their CPUs, all the way up, scans through all these SSDs, into RAM, into their CPU, does the very little bit of math it needs to. And then after it does all that across
[82:56] the whole network of machines, it all gets spat back over the network to my machine. And I receive a couple of numbers, which is this many. Let's say I now, from the, from the hundreds of billions of
[83:08] events I started with, trillions of events I started with, I now know there's only about three or four million I care about, which could be gamma ray bursts. So I go, that's not too bad.
[83:19] I can return some of that data over the wire and it won't kill my machine, right? I can't pull petabytes over the wire. I'll be here for months, generally months, right? So instead, I now know
[83:31] I'm not going to do something crazy. So I say, select now rather than count. So select is going to get me some data. We're in dangerous territory now. Select, select, and these are the columns I
[83:44] care about. So I now know they're into the spectrum of the things I care about. So now I want to know where they are in the sky. So which direction they're in, which is like a series of angles
[83:53] and some other bits and pieces, right? So which direction that signal came from. So I say select the direction column, the intensity column, the wavelength column, and there's probably a distance
[84:07] column there as well. Like how far away was it? Okay. So those four columns, select all of those, please, where I'm inside this field. Now, Clickhouse crunches through all this again.
[84:19] This takes a bit longer because it's now returning more data. So it's going to pull more data into the CPU, send it over the wire. And over the next, you know, 150 seconds, let's say a couple of
[84:29] minutes go by, and I get a couple of gigabytes of data locally. It'd be five to six gigabytes of data, right? Which is about realistic, to be honest. It takes about five minutes to download
[84:38] a few gigabytes. And I now have this one big table of data. And I think that is cool. I now have something I can now work with locally. And the reason I had to do it remotely was because one,
[84:49] I want to use all the CPUs on all those different machines. But two, because that data needed to be done locally. So it was fast. This is why Clickhouse is really fast. This is what's known,
[85:00] the data is right next to the RAM. It is right next to the CPU as it runs on. That calculation was done really close to the data. And then we spat up only the bit we care
[85:14] about over the wire. >> You are definitely giving me more questions for next time. Because I know we're close to wrapping time. And I also feel like a lot of this information I already knew,
[85:34] but they weren't together. So it didn't necessarily make sense. But at the same time, I'm like, if you keep adding things, I'm going to start giving you a blank stare.
[85:44] >> So, this is the kind of core concept here is the things that you'll find that these patterns repeat. And this is my favorite thing here is at a certain point, you'll get to this weird
[85:57] realization that it's all the same. And genuinely, and like the three things that always end up mattering are how easy is it to find the data I care about? Like how can I search for it? So,
[86:08] this is indexing. This is acceleration structures. This is core of like how do I get to the thing I need fast? So, I only go there. Right? Then it's how do I load it as fast as possible into a place
[86:24] I can crunch it? And then it's how do I get only what I care about? So, what I load is small enough that it fits. So, it's how do I find the bit I care about? How do I access it as fast as I can?
[86:37] And how do I keep as little as possible around? >> And that all makes sense. >> And those are the only three things that matter in most things to do with data.
[86:45] Databases, rendering, all the same. Right? It's I need to find it. I need to get it to where it needs to be. And I need to be as small as possible so I can get past it as fast as possible.
[86:57] Everything else is just a way of getting there. >> Okay. I will get there. Like what you say makes sense. But connecting. >> Yes. There's a lot of dots to connect here. And the thing,
[87:16] so, the core thing to always remember is we always want to go a bit faster. And we don't want to wait once we are achieving what we want to wait. What we wanted. Same reason as we go back
[87:26] to Napster again. Napster is a wonderful example. Right? Napster in the days of dial-up, the slowest part was what we had to wait for. So, we did it as a batch. We chunked it over the wire overnight.
[87:37] I remember from the similar timelines. We set it going and we waited and we came back some hours later or longer hoping no one else picked up the phone in the meantime. Oh, yeah. I remember those
[87:51] days as well. And eventually on our machine we had a file which would work in theory. But the point was that we then wanted an uninterrupted experience. We had to wait for the slow bit to happen so we
[88:03] could do the fast bit. Right? >> That makes sense. >> Yeah. So, we searched for the song we wanted. That was fairly fast. We were just searching for metadata. We looked for what we cared about. We
[88:13] said what's the name of the song we care about? And we want to find that one track that we care about. So, we found that track. That took a little bit of time. Then we said go. We chunked it over
[88:24] the very slow bit of the network and then we had it locally. Then we could experience it. Which is that it goes up the stack again and hits the thing we care about. >> Okay. And that's actually
[88:36] starting to make a lot more sense. It's just... >> It's all the same. It's just are you the database or is the database the database? >> Yeah. And I think this goes into a lot of also
[88:49] how are we processing the data and what eventually what we do with the data. Like what are things that you can use databases for? Those are all like future questions. Because I know we definitely did
[89:02] not get anywhere near those. So, lots to go over in the future. And thank you for coming on the stream today. Anything else that I know you gave us the three things to know is how to find the
[89:17] data, how to what's the fastest way to load the data, and how do I get only what I want from the data? So, other than those three things, anything else you want us to walk away with? >> That,
[89:32] as I said, the key thing there is it's literally the same thing applied everywhere. And I can walk you through why a game engine does exactly the same thing as a database, as Kafka, as your file
[89:44] system, and your offerings. It's all the same. And genuinely all the same. >> We will get there. We will get there. But thank you again for joining today. And y'all, if you want to find Ben on
[89:58] Twitter, his Twitter handle is right below his video. But also it is Ben Gamble 7. >> Yes. I'm lazy. I use my name everywhere. Because otherwise I will forget my username. >> I mean, I have the
[90:13] show Teach Gen Tech, yet everything is under my name, Jen Schnod. Because I'm like, I don't, that's not going to change. At least. So, thank you, everyone.
[90:26] [BLANK_AUDIO] 
